from transformers import DebertaV2Tokenizer, DebertaV2Model
import torch
from sklearn.metrics.pairwise import cosine_similarity

# Load the pre-trained DeBERTa model and tokenizer
tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-base')
model = DebertaV2Model.from_pretrained('microsoft/deberta-v3-base')

# Define PESTLE categories and their representative keywords
PESTLE_categories = {
    "Political": "policy, governance, government, regulation",
    "Economic": "economy, financial, income, poverty",
    "Social": "community, social, people, demographic",
    "Technological": "technology, infrastructure, innovation, energy",
    "Legal": "law, legislation, compliance, rights",
    "Environmental": "environment, ecology, natural, disaster, climate"
}

# Convert category keywords to embeddings (average of keywords)
def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).detach().numpy()

category_embeddings = {category: get_embedding(keywords) for category, keywords in PESTLE_categories.items()}

# Input sentences
sentences = [
"""sentence 1""", 
"""sentence 2""",
"""sentence 3""",
...
more sentences
]

# Calculate embeddings for each sentence
sentence_embeddings = [get_embedding(sentence) for sentence in sentences]

# Calculate cosine similarity for each sentence with each PESTLE category
results = []
for i, sentence in enumerate(sentences):
    sentence_scores = {category: cosine_similarity(sentence_embeddings[i], category_embedding).item() 
                       for category, category_embedding in category_embeddings.items()}
    # Sort scores for clarity
    sorted_scores = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)
    results.append({"Sentence": sentence, "Scores": sorted_scores})

# Output results
for result in results:
    print(result)
